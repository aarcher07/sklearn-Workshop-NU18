{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop is divided into 5 sections.  In section 1, I will be dicussing data preprocessing. In section 2, I will be discussing regression techniques. In section 3, I will be discussing dimensionality reduction algorithms. In section 4, I will be discussing classification. In section 5, I will be discussing model selection.\n",
    "\n",
    "\n",
    "All these topics will be taught in Python and use the machine learning package, [scikit-learn](http://scikit-learn.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sections 1 and 2, I will be using a dataset of avocado prices and volume sold in U.S. cities.\n",
    "\n",
    "For sections 3,4 and 5, I will be using a dataset of antibotic resistance in gonorrhea strains.\n",
    "\n",
    "**Note: In addition to sklearn, this workshop requires pandas and matplotlib. Although not required, I would be great if you had graphviz.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from CompBioProject import functions\n",
    "import matplotlib\n",
    "import graphviz\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avocado U.S. Cities Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains avocado prices and total volume sold in U.S. cities. [I obtain the data from the website, Kaggle.](https://www.kaggle.com/neuromusic/avocado-prices-across-regions-and-seasons/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocado_path = '/Users/aarcher/Downloads/avocado.csv'\n",
    "avocado_df = pd.read_csv(avocado_path,header=0)\n",
    "avocado_df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The avocado dataset is stored as a [pandas dataframe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html), **avocado_df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataframe has columns: ',avocado_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Kaggle, the description of the columns are:\n",
    "\n",
    "- Date - The date of the observation\n",
    "- AveragePrice - the average price of a single avocado\n",
    "- type - conventional or organic\n",
    "- year - the year\n",
    "- Region - the city or region of the observation\n",
    "- Total Volume - Total number of avocados sold\n",
    "- 4046 - Total number of avocados with PLU 4046 sold\n",
    "- 4225 - Total number of avocados with PLU 4225 sold\n",
    "- 4770 - Total number of avocados with PLU 4770 sold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load the dataset below and add a column for month of observation. I also print the \"head\" of the dataset. The head is the first n entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = [int(date[5:7]) for date in avocado_df.loc[:,'Date'].values]\n",
    "avocado_df.loc[:,'Month'] = month\n",
    "print(avocado_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be apply the preprocessing techniques:\n",
    "    \n",
    "- Normalization\n",
    "- Standardization\n",
    "- Label Encoding\n",
    "- Training-Test Split\n",
    "\n",
    "to the avocado dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through the cells below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create the **total_volume_and_bags** variable to store the Total Volume and Total Bags columns. \n",
    "\n",
    "We'll normalize and standardize these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_volume_and_bags = avocado_df.loc[:,['Total Volume','Total Bags']]\n",
    "print('The first 5 entries of total volume are:\\n',total_volume_and_bags.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1\n",
    "\n",
    "I have already normalize total_volume_and_bags and print its value. See the below.\n",
    "\n",
    "Standardize **total_volume_and_bags** using an instance of StandardScaler. Print the standardized variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer_scaler = Normalizer()\n",
    "normalized_volume_bags = normalizer_scaler.fit_transform(total_volume_and_bags)\n",
    "print('The head of the normalized Total Volume and Total Bags are:\\n',normalized_volume_bags[0:5,:])\n",
    "print('\\n')\n",
    "standardized_scaler = StandardScaler()\n",
    "standardized_volume_bags = standardized_scaler.fit_transform(total_volume_and_bags)\n",
    "print('The standardized Total Volume and Total Bags are:\\n',standardized_volume_bags[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the categorical variables, region and type, in regression, we must encode them using integers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **region_categories** and **type_categories variables** store the unique categories in the region and type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_categories = avocado_df.loc[:,'region'].unique()\n",
    "type_categories = avocado_df.loc[:,'type'].unique()\n",
    "\n",
    "print('Region categories are: ',region_categories,'\\n')\n",
    "print('Type categories are: ',type_categories,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2\n",
    "\n",
    "I have already encoded **region_categories**. Using code for encoding **region_categories** as an example, encode the **type_categories** using an instance of LabelEncoder and print the encoded type categories. \n",
    "\n",
    "Note: you must use different instances of LabelEncoder to encode region_categories and type_categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "region_encoder = LabelEncoder()\n",
    "encoded_region_cats = region_encoder.fit_transform(region_categories)\n",
    "print('The encoded region categories are:', encoded_region_cats)\n",
    "print('\\n')\n",
    "\n",
    "type_encoder  = LabelEncoder()\n",
    "encoded_type_cats = type_encoder.fit_transform(type_categories)\n",
    "print('The encoded types are:', encoded_type_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode the region and type column run the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before the region column was: \\n', avocado_df.loc[:,'region'].head())\n",
    "print('\\n')\n",
    "print('Before the type column was: \\n', avocado_df.loc[:,'type'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS ONLY ONCE. IF YOU RUN IT TWICE OR MORE, YOU WILL GET AN ERROR ##\n",
    "\n",
    "avocado_df.loc[:,'region'] = region_encoder.transform(avocado_df.loc[:,'region'])\n",
    "avocado_df.loc[:,'type'] = type_encoder.transform(avocado_df.loc[:,'type'])\n",
    "\n",
    "## RUN THIS ONLY ONCE. IF YOU RUN IT TWICE OR MORE, YOU WILL GET AN ERROR ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After the region column is now: \\n', avocado_df.loc[0:5,'region'])\n",
    "print('\\n')\n",
    "print('After the type column is now: \\n', avocado_df.loc[0:5,'type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be attempting to predict price of an avocado given the demand, time of year and place of purchase. We will then using the columns, '4046', '4225', '4770', 'Small Bags', 'Large Bags', 'XLarge Bags', 'type'\n",
    " 'year', 'region', 'Month', as explanatory variables and the price as the price variable.\n",
    " \n",
    "I use **avocado_explanatory_variables** to store the explanatory variables and **avocado_response_variables** stores the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocado_response_variables = avocado_df.loc[:,'AveragePrice']\n",
    "avocado_explanatory_variables = avocado_df.drop(['Date','Total Volume','Total Bags','AveragePrice'], axis=1)\n",
    "\n",
    "print('avocado_explanatory_variables stores: \\n',avocado_explanatory_variables.loc[0:5,:])\n",
    "print('\\n')\n",
    "print('avocado_response_variables stores: \\n',avocado_response_variables.loc[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I standardized the columns of the avocado_explanatory_variables, except the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RUN ONLY ONCE. YOU WILL GET AN ERROR IF YOU RUN TWICE ###\n",
    "standardized_scaler_explanatory= StandardScaler()\n",
    "standardized_scaler_response= StandardScaler()\n",
    "avocado_standardized_explanatory_columns = standardized_scaler_explanatory.fit_transform(\n",
    "                                            avocado_explanatory_variables.loc[:,['4046', \n",
    "                                            '4225', '4770', 'Small Bags', 'Large Bags', 'XLarge Bags']])\n",
    "\n",
    "avocado_explanatory_variables.loc[:,['4046', '4225', '4770', 'Small Bags', \n",
    "                                     'Large Bags', 'XLarge Bags']] =  avocado_standardized_explanatory_columns\n",
    "\n",
    "print('The standard explanatory variables are: \\n\\n', avocado_explanatory_variables.loc[0:5,:],'\\n\\n')\n",
    "#### RUN ONLY ONCE. YOU WILL GET AN ERROR IF YOU RUN TWICE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I use train_test_split to split\n",
    "-  avocado_explanatory_variables into training_set, test_set\n",
    "-  avocado_response_variables y_training_set and y_test_set.\n",
    "\n",
    "I use a $70\\%:30\\%$ training to test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "training_set, test_set, y_training_set, y_test_set = train_test_split(avocado_explanatory_variables,\n",
    "                                                                      avocado_response_variables,test_size=0.30,\n",
    "                                                                      train_size=0.70,random_state=0)\n",
    "\n",
    "print('The training division of the explanatory variables has head: \\n\\n',training_set.head(),'\\n\\n')\n",
    "print('The training division of the response variables has head: \\n\\n',y_training_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.3\n",
    "\n",
    "The Avocado dataset has 18248 observation. Instead of splitting the data set using a fraction, do $70\\%:30\\%$ split using integers in the arguments: train_size and test_size.\n",
    "\n",
    "That is, change train_size and test_size to integers so that we get a 70\\%:30\\% split.\n",
    "\n",
    "Note: $18248 * 30\\% \\approx 5474$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, test_set, y_training_set, y_test_set = train_test_split(avocado_explanatory_variables,\n",
    "                                                                      avocado_response_variables,\n",
    "                                                                      test_size=5474,train_size=18248-5474,\n",
    "                                                                      random_state=0)\n",
    "print('The training division of the explanatory variables has head: \\n\\n',training_set.head(),'\\n\\n')\n",
    "print('The training division of the response variables has head: \\n\\n',y_training_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in training_set.columns.values:\n",
    "    plt.scatter(training_set.loc[:, label],y_training_set.values)\n",
    "    plt.xlabel('Normalized ' + label)\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Price vs. Normalized '+ label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an instance of LinearRegression, I indiviually regress each columns of the training_set against y_training_set and plot the best fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "column_names = avocado_explanatory_variables.columns.values\n",
    "\n",
    "for label in column_names:\n",
    "    x_training = training_set.loc[:,label].values.reshape(-1,1)\n",
    "    x_test = test_set.loc[:,label].values.reshape(-1,1)\n",
    "    lr.fit(x_training,y_training_set)\n",
    "    m = lr.coef_\n",
    "    b = lr.intercept_\n",
    "    \n",
    "    plt.scatter(x_training,y_training_set.values)\n",
    "    plt.plot(x_training, b + m * x_training, 'r-')\n",
    "    plt.xlabel('Training set - Normalized ' + label)\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Training set : Price vs. Normalized '+ label + ', $R^2=$'\n",
    "              +str(lr.score(x_training,y_training_set)))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(x_test,y_test_set.values)\n",
    "    plt.plot(x_test, b + m * x_test, 'r-')\n",
    "    plt.xlabel('Training set - Normalized ' + label)\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Test set : Price vs. Normalized '+ label + ', $R^2=$'\n",
    "              +str(lr.score(x_test,y_test_set)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, a linear model performs pretty poorly in each instance.\n",
    "\n",
    "What if we just threw all the variables in a linear model? How would the linear model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(training_set,y_training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1\n",
    "\n",
    "Print the coefficients of Linear Model with all variables. Also print the $R^2$ value of the model using the training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('The coefficients, except the intercept, are: ', lr.coef_,'\\n')\n",
    "print('The intercept is: ',lr.intercept_,'\\n')\n",
    "print('The R^2 from the training set is: ', str(lr.score(training_set,y_training_set)), '\\n')\n",
    "print('The R^2 from the training set is: ',str(lr.score(test_set,y_test_set)),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more variables, the model performs a bit better! However, the model is far from perfect.\n",
    "\n",
    "Could LASSO or Rigid regression help us? \n",
    "\n",
    "They're worth a try.\n",
    "\n",
    "### LASSO and Rigid Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an instance of LASSO with alpha=1.0, I regress the columns of training_set against y_training_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=1)\n",
    "lasso.fit(training_set,y_training_set)\n",
    "\n",
    "print('The coefficients, except the intercept, are: ', lasso.coef_,'\\n')\n",
    "print('The intercept is: ',lasso.intercept_,'\\n')\n",
    "print('The R^2 from the training set is: ', str(lasso.score(training_set,y_training_set)), '\\n')\n",
    "print('The R^2 from the training set is: ',str(lasso.score(test_set,y_test_set)),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, LASSO doesn't help. It actually performs worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2 \n",
    "\n",
    "Now it's your turn!\n",
    "\n",
    "Using an instance of Rigid with alpha=1.0, regress the columns of training_set against y_training_set. Print the coefficients, interpet and $R^2$ with the training and test set. Comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(training_set,y_training_set)\n",
    "\n",
    "print('The coefficients, except the intercept, are: ', ridge.coef_,'\\n')\n",
    "print('The intercept is: ',ridge.intercept_,'\\n')\n",
    "print('The R^2 from the training set is: ', str(ridge.score(training_set,y_training_set)), '\\n')\n",
    "print('The R^2 from the training set is: ',str(ridge.score(test_set,y_test_set)),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "A linear model is just wrong for this problem.\n",
    "\n",
    "What about using decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use an instance of DecisionTreeRegressor with max depth 5 to regress training_set against y_training_set. \n",
    "\n",
    "I print the feature importance and $R^2$ value using training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor(max_depth=5,random_state=0)\n",
    "dtr.fit(training_set,y_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The R^2 value for training set is: ',dtr.score(training_set,y_training_set))\n",
    "print('The R^2 value for test set is: ',dtr.score(test_set,y_test_set))\n",
    "print('\\n')\n",
    "for feature,importance in zip(avocado_explanatory_variables.columns.values.tolist(),dtr.feature_importances_):\n",
    "    print(feature,'variable has importance,', importance,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plot tree produced below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import graphviz \n",
    "import sklearn.tree as tree\n",
    "dot_data = tree.export_graphviz(dtr, out_file=None, \n",
    "                         feature_names=column_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the max depth to 10, unsurprisingly, the model accuracy **increases**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor(max_depth=10,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtr.fit(training_set,y_training_set)\n",
    "print('The R^2 value for training set is: ',dtr.score(training_set,y_training_set))\n",
    "print('The R^2 value for test set is: ',dtr.score(test_set,y_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a random forest regression, we see similar successes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=10,max_depth=10,max_features=5,random_state=0)\n",
    "rf.fit(training_set,y_training_set)\n",
    "\n",
    "print('The R^2 value for training set is: ',rf.score(training_set,y_training_set))\n",
    "print('The R^2 value for test set is: ',rf.score(test_set,y_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3\n",
    "\n",
    "Play around with the max depth parameter for the decision tree and random forest regression. What parameter gives the lowest error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor(max_depth=20,random_state=0)\n",
    "dtr.fit(training_set,y_training_set)\n",
    "print('The R^2 value for training set is: ',dtr.score(training_set,y_training_set))\n",
    "print('The R^2 value for test set is: ',dtr.score(test_set,y_test_set))\n",
    "print('\\n')\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=10,max_depth=20,max_features=5,random_state=0)\n",
    "rf.fit(training_set,y_training_set)\n",
    "\n",
    "print('The R^2 value for training set is: ',rf.score(training_set,y_training_set))\n",
    "print('The R^2 value for test set is: ',rf.score(test_set,y_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Mer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data from [PATRIC](https://www.patricbrc.org/), I created a puedo-genomes of *Neisseria Gonorrhea* bacteria strains. Each genome is labelled for their antibotic resistance to *azithromycin*.\n",
    "\n",
    "With each strain, I splice the DNA in k-mers. k-mers are consecutive cuts of a DNA strand which contains *k* nucleotides.\n",
    "\n",
    "The image below shows the 7-mers of ATGGAAGTCGCGGAATC.\n",
    "\n",
    "![7mers.png](attachment:7mers.png)\n",
    "\n",
    "I collected all possible unqiue k-mer from each genome. I then constructed a dataset whose rows represented a strain and columns represented a k-mer. A k-mer column had 0 if the strain's genome did not contain the k-mer and 1 if the strain's genome contained the k-mer.\n",
    "\n",
    "A genome is label 1 if it is suspectible to *azithromycin* and 0 if it is resistant to *azithromycin*.\n",
    "\n",
    "**I aim to build a classifer that can correctly label antibotic resistance in *Neisseria Gonorrhea*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I load the k-mers data set. I have already divided the set into training set and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"2018-07-15T18\"\n",
    "kmers_training_set = functions.load_obj(address + \"/train\").todense()\n",
    "label_training_set = functions.load_obj(address + \"/label_train\")\n",
    "kmers_test_set = functions.load_obj(address + \"/test\").todense()\n",
    "label_test_set = functions.load_obj(address + \"/label_test\")\n",
    "\n",
    "print('The training set has shape: ',kmers_training_set.shape)\n",
    "print('The training set has shape: ',kmers_test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dimensionality Reduction: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I apply PCA without whitening to the k-mer training data set and reduce the number of features (dimensions) to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_without_whitening = PCA(n_components=2,whiten=False)\n",
    "pca_without_whitening.fit(kmers_training_set)\n",
    "\n",
    "kmers_training_pca_without_whitening = pca_without_whitening.transform(kmers_training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I generate a two dimensional plot of PCA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colours = ['red','blue']\n",
    "\n",
    "plt.scatter(kmers_training_pca_without_whitening[:, 0],kmers_training_pca_without_whitening[:, 1],\n",
    "            c=label_training_set, cmap=matplotlib.colors.ListedColormap(colours))\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.title('PCA plot of k-mer training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also apply learn PCA without whitening, learned from the training set, to the k-mer test data set and reduce the number of features (dimensions) to 2. \n",
    "\n",
    "I then plot test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_test_pca_without_whitening = pca_without_whitening.transform(kmers_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colours = ['red','blue']\n",
    "\n",
    "plt.scatter(kmer_test_pca_without_whitening[:, 0],kmer_test_pca_without_whitening[:, 1],\n",
    "            c=label_test_set, cmap=matplotlib.colors.ListedColormap(colours))\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.title('PCA plot of k-mer test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then print the explained variance and singular values for each singular value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(pca_without_whitening.explained_variance_)\n",
    "for i in range(num):\n",
    "    print('Component',i, 'explains',pca_without_whitening.explained_variance_[i],'variance')\n",
    "    print('Component',i, 'has singular value', pca_without_whitening.singular_values_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "Using the variables kmers_training_set_dense and kmers_test_set_dense, apply PCA with whiten to the k-mer training and test data set. The variables are initialized in the cell below.\n",
    "\n",
    "Store the reduced training in the variable, kmer_train_pca_with_whitening, and the reduced training in the variable, kmer_test_pca_with_whitening.\n",
    "\n",
    "Run the below code to generate a plot. Comment on the differences between the whitened and non-whitened plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_with_whitening = PCA(n_components=2,whiten=True)\n",
    "kmers_training_pca_with_whitening = pca_with_whitening.fit_transform(kmers_training_set)\n",
    "\n",
    "kmer_test_pca_with_whitening = pca_with_whitening.transform(kmers_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = ['red','blue']\n",
    "\n",
    "plt.scatter(kmers_training_pca_with_whitening[:, 0],kmers_training_pca_with_whitening[:, 1],\n",
    "            c=label_training_set, cmap=matplotlib.colors.ListedColormap(colours))\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.title('PCA plot of k-mer training data')\n",
    "plt.show()\n",
    "\n",
    "colours = ['red','blue']\n",
    "\n",
    "plt.scatter(kmer_test_pca_with_whitening[:, 0],kmer_test_pca_with_whitening[:, 1],\n",
    "            c=label_test_set, cmap=matplotlib.colors.ListedColormap(colours))\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.title('PCA plot of k-mer test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I build a Native Bayes classifer to learn and predict resistance in the genomes. \n",
    "\n",
    "I print the model parameters for the first 10 k-mers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gNB=GaussianNB()\n",
    "gNB.fit(kmers_training_set,label_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.8813559322033898\n",
      "\n",
      "\n",
      "0 has 150.0 classes\n",
      "1 has 124.0 classes\n",
      "\n",
      "\n",
      "73016\n",
      "When class = 0, k-mer 0 has mean 0.0\n",
      "When class = 0, k-mer 0 has variance 2.5e-10\n",
      "\n",
      "\n",
      "When class = 1, k-mer 0 has variance 0.024193548387096774\n",
      "When class = 1, k-mer 0 has variance 0.02360822085353792\n",
      "\n",
      "\n",
      "When class = 0, k-mer 1 has mean 0.0\n",
      "When class = 0, k-mer 1 has variance 2.5e-10\n",
      "\n",
      "\n",
      "When class = 1, k-mer 1 has variance 0.024193548387096774\n",
      "When class = 1, k-mer 1 has variance 0.02360822085353792\n",
      "\n",
      "\n",
      "When class = 0, k-mer 2 has mean 0.0\n",
      "When class = 0, k-mer 2 has variance 2.5e-10\n",
      "\n",
      "\n",
      "When class = 1, k-mer 2 has variance 0.024193548387096774\n",
      "When class = 1, k-mer 2 has variance 0.02360822085353792\n",
      "\n",
      "\n",
      "When class = 0, k-mer 3 has mean 0.0\n",
      "When class = 0, k-mer 3 has variance 2.5e-10\n",
      "\n",
      "\n",
      "When class = 1, k-mer 3 has variance 0.024193548387096774\n",
      "When class = 1, k-mer 3 has variance 0.02360822085353792\n",
      "\n",
      "\n",
      "When class = 0, k-mer 4 has mean 0.0\n",
      "When class = 0, k-mer 4 has variance 2.5e-10\n",
      "\n",
      "\n",
      "When class = 1, k-mer 4 has variance 0.024193548387096774\n",
      "When class = 1, k-mer 4 has variance 0.02360822085353792\n",
      "\n",
      "\n",
      "When class = 0, k-mer 5 has mean 0.0\n",
      "When class = 0, k-mer 5 has variance 2.5e-10\n",
      "\n",
      "\n",
      "When class = 1, k-mer 5 has variance 0.024193548387096774\n",
      "When class = 1, k-mer 5 has variance 0.02360822085353792\n",
      "\n",
      "\n",
      "When class = 0, k-mer 6 has mean 0.0\n",
      "When class = 0, k-mer 6 has variance 2.5e-10\n",
      "\n",
      "\n",
      "When class = 1, k-mer 6 has variance 0.024193548387096774\n",
      "When class = 1, k-mer 6 has variance 0.02360822085353792\n",
      "\n",
      "\n",
      "When class = 0, k-mer 7 has mean 0.0\n",
      "When class = 0, k-mer 7 has variance 2.5e-10\n",
      "\n",
      "\n",
      "When class = 1, k-mer 7 has variance 0.024193548387096774\n",
      "When class = 1, k-mer 7 has variance 0.02360822085353792\n",
      "\n",
      "\n",
      "When class = 0, k-mer 8 has mean 0.0\n",
      "When class = 0, k-mer 8 has variance 2.5e-10\n",
      "\n",
      "\n",
      "When class = 1, k-mer 8 has variance 0.024193548387096774\n",
      "When class = 1, k-mer 8 has variance 0.02360822085353792\n",
      "\n",
      "\n",
      "When class = 0, k-mer 9 has mean 0.0\n",
      "When class = 0, k-mer 9 has variance 2.5e-10\n",
      "\n",
      "\n",
      "When class = 1, k-mer 9 has variance 0.024193548387096774\n",
      "When class = 1, k-mer 9 has variance 0.02360822085353792\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gNB.score(kmers_training_set,label_training_set))\n",
    "print(gNB.score(kmers_test_set,label_test_set))\n",
    "results = [int(i) for i in label_training_set]\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(gNB.class_count_)):\n",
    "    print(i,'has',gNB.class_count_[i], 'classes')\n",
    "    \n",
    "print('\\n')\n",
    "print(len(gNB.theta_[0,:]))\n",
    "for i in range(10):\n",
    "    print('When class = 0, k-mer', i,'has mean',gNB.theta_[0,i])\n",
    "    print('When class = 0, k-mer', i,'has variance',gNB.sigma_[0,i])\n",
    "    print('\\n')\n",
    "    print('When class = 1, k-mer', i,'has variance',gNB.theta_[1,i])\n",
    "    print('When class = 1, k-mer', i,'has variance',gNB.sigma_[1,i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Native Bayes performs fairly well on predict antibotic resistance. It has $88\\%$ accuracy rate on the test set.\n",
    "\n",
    "Below, I also construct the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the training set,\n",
      " [[150   0]\n",
      " [  0 124]]\n",
      "On the training set,\n",
      " [[56  8]\n",
      " [ 6 48]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predict_label_train_set = gNB.predict(kmers_training_set)\n",
    "predict_label_test_set = gNB.predict(kmers_test_set)\n",
    "\n",
    "print('On the training set,\\n',confusion_matrix(label_training_set,predict_label_train_set))\n",
    "print('On the training set,\\n',confusion_matrix(label_test_set,predict_label_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1\n",
    "Apply Native Bayes to the PCA data and print confusion matrix for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr=LogisticRegression()\n",
    "lgr.fit(kmers_training_set_dense,label_training_set)\n",
    "print(lgr.coef_) #TODO: CHANGE IN PRESENTATION\n",
    "print(lgr.intercept_)\n",
    "print(lgr.score(kmers_training_set_dense,label_training_set))\n",
    "print(lgr.score(kmers_test_set_dense,label_test_set))\n",
    "predict_label_train_set_lgr = lgr.predict(kmers_training_set_dense)\n",
    "predict_label_test_set_lgr = lgr.predict(kmers_test_set_dense)\n",
    "print(confusion_matrix(label_training_set,predict_label_train_set_lgr))\n",
    "print(confusion_matrix(label_test_set,predict_label_test_set_lgr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2\n",
    "Apply Logistic Regression to the PCA data and print confusion matrix for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(max_depth=None,random_state=0)\n",
    "dtc.fit(kmers_training_set_dense,label_training_set)\n",
    "#for feature,importance in zip(avocado_explanatory_variables.columns.values.tolist(),dtc.feature_importances_):\n",
    "#    print(feature,'variable has importance,', importance,'\\n')\n",
    "print(dtc.feature_importances_)\n",
    "print('The accuracy for training set is: ',dtc.score(kmers_training_set_dense,label_training_set))\n",
    "print('The accuracy for test set is: ',dtc.score(kmers_test_set_dense,label_test_set))\n",
    "predict_label_train_set_lgr = dtc.predict(kmers_training_set_dense)\n",
    "predict_label_test_set_lgr =  dtc.predict(kmers_test_set_dense)\n",
    "print(confusion_matrix(label_training_set,predict_label_train_set_lgr))\n",
    "print(confusion_matrix(label_test_set,predict_label_test_set_lgr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3\n",
    "\n",
    "Apply a decision tree to the PCA data and print confusion matrix for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                              n_estimators=50, random_state=0)\n",
    "adaboost.fit(kmers_training_set_dense,label_training_set)\n",
    "#for feature,importance in zip(avocado_explanatory_variables.columns.values.tolist(),dtc.feature_importances_):\n",
    "#    print(feature,'variable has importance,', importance,'\\n')\n",
    "print(adaboost.feature_importances_)\n",
    "print('The accuracy for training set is: ',adaboost.score(kmers_training_set_dense,label_training_set))\n",
    "print('The accuracy for test set is: ',adaboost.score(kmers_test_set_dense,label_test_set))\n",
    "predict_label_train_set_ada = adaboost.predict(kmers_training_set_dense)\n",
    "predict_label_test_set_ada =  adaboost.predict(kmers_test_set_dense)\n",
    "print(confusion_matrix(label_training_set,predict_label_train_set_ada))\n",
    "print(confusion_matrix(label_test_set,predict_label_test_set_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.4\n",
    "Apply Adaboost to the PCA data and print confusion matrix for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(kmers_training_set_dense,label_training_set)\n",
    "#for feature,importance in zip(avocado_explanatory_variables.columns.values.tolist(),dtc.feature_importances_):\n",
    "#    print(feature,'variable has importance,', importance,'\\n')\n",
    "print('The accuracy for training set is: ',knn.score(kmers_training_set_dense,label_training_set))\n",
    "print('The accuracy for test set is: ',knn.score(kmers_test_set_dense,label_test_set))\n",
    "predict_label_train_set_knn = knn.predict(kmers_training_set_dense)\n",
    "predict_label_test_set_knn =  knn.predict(kmers_test_set_dense)\n",
    "print(confusion_matrix(label_training_set,predict_label_train_set_knn))\n",
    "print(confusion_matrix(label_test_set,predict_label_test_set_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.5\n",
    "Apply KNN to the PCA data and print confusion matrix for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
